<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Nutch, HDFS, and Elasticsearch on AWS t2.micro instances (Free Tier) &middot; Dr. Mario
    
  </title>

  <!-- favicon -->
  <link rel="icon" href="/pics/favicon.ico">

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="/public/css/own.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">

  <!-- JS -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.2.2/jquery.min.js"></script>

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


  <body class="theme-base-18">

    <div class="sidebar">
  <div class="container">
    <div class="sidebar-about">
      <h1>
          Dr. Mario
      </h1>
      
      <img class="sidebar-img" src="/pics/Dr_Mario.jpg" alt="Dr. Mario">
      
      
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/about_me">About me</a> 
      <a class="sidebar-nav-item" href="/cv">CV</a>
      <a class="sidebar-nav-item" href="/portfolio">Portfolio</a>
      <a class="sidebar-nav-item" href="/">Blog</a>
      <a class="sidebar-nav-item" href="/quantified_self">Quantified self</a>
      <a class="sidebar-nav-item" href="/target_list">Target List </a>
    </nav>
    <p class="contact-icons">
      <a class="fa fa-github" href="https://github.com/Mario-LopezTelecom"></a>
      <a class="fa fa-linkedin" href="https://es.linkedin.com/in/mariolopezmartinez"></a>
      <a class="fa fa-envelope-o" href="mailto:mario.lopezTelecom@gmail.com"></a>
    </p>
  </div>
</div>


    <div class="content container">
      <div class="post">
  <h1 class="post-title">Nutch, HDFS, and Elasticsearch on AWS t2.micro instances (Free Tier)</h1>
  <span class="post-date">26 Jul 2016</span>
  <p>As a small exercise I committed to see how far I could go into the following task definition: </p>

<blockquote>
<p>Configure an HTTP crawler on Amazon AWS (Free Tier) with high performance and index a couple of websites.</p>

<ul>
<li> Use Nutch as crawler.</li>
<li> Configure it to use several machines simultaneously for crawling.</li>
<li> Configure HDFS as filesystem. </li>
<li> Use ElasticSearch for storage and indexing.</li>
</ul>
</blockquote>

<!--more-->

<p>And this is what I got after a week of work and a total of 15 hours:
<img style="display:block;margin:auto" src="/pics/2016-06-26/system_diagram.png" alt=""/></p>

<ul>
<li> Three Amazon EC2 t2.micro instances running Hadoop 2.6.4: one as master and two worker nodes.</li>
<li> Apache Nutch 1.12 successfully running on top of the Hadoop cluster.</li>
<li> An Elasticsearch cluster of two nodes successfully indexing the results of the Nutch crawl (5 shards and 1 replica).</li>
</ul>

<p>Which we can compare to my initial estimation:</p>

<ul>
<li> <code>[COMPLETED]</code>. Nutch running on top of a 3-machine Hadoop cluster (85% chance of success).</li>
<li> <code>[COMPLETED]</code>. ElasticSearch integration (65% chance of success).</li>
<li> <code>[NOT COMPLETED]</code>. Small study regarding performance (40% chance of success).</li>
</ul>

<p>This means that my final performance got pretty close to the expected results.</p>

<h1>1. Mini guide and references on how to set-up the system</h1>

<h3>1.1. Hadoop</h3>

<p>1) Spin up three t2.micro instances with the following Amazon Machine Image: Amazon Linux AMI 2016.03.2 (HVM), SSD Volume Type - ami-f303fb93. No need to change anything on the default configuration, but keep all three instances within the same Security Group [1].</p>

<p>2) In addition to the default SSH rule, include the following inbound rules in the Security Group (Type, Protocol, Port Range, Source):</p>

<ul>
<li> All traffic, All, All, <code>$your_security_group</code>: this allows the three machines to communicate with each other</li>
<li> Custom TCP, TCP, 50700, <code>$ips_for_web_access</code>.</li>
<li> Same than above for ports 8088, 19888: these last two rules will allow you to access the web UI of ResourceManager and JobHistory Server to track the state of jobs and nodes (from the computers within `$ips<em>for</em>web_access).</li>
</ul>

<p>3) Download Hadoop 2.6.4 binaries from [3]. Configure it according to the this gist [4]. Helpful resources on this step are [5-7]. Only a few remarks are needed for a configuration on different machines:</p>

<ul>
<li> <code>core_site.xml/fs.defaultFS</code>. This address is your address of the node that will act as NameNode.</li>
<li> <code>slaves</code>. Change the address in there for the address of your worker nodes.</li>
<li> <code>yarn_site.xml/yarn.resourcemanager.hostname</code>. This is the address of the node that will act as ResourceManager.</li>
</ul>

<p>4) Before start using the dfs, we have to format it by running the following in the master node:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$HADOOP_HOME/bin/hdfs namenode -format $cluster_name
</code></pre></div>
<p>where <code>$cluster_name</code> is a name of your choice.</p>

<p>5) At this point, the hdfs (and YARN) should work. We can start all the required processes in all machines by running on the master node:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$HADOOP_HOME/sbin/start-dfs.sh
$HADOOP_HOME/sbin/start-yarn.sh
$HADOOP_HOME/sbin/mr-jobhistory-daemon.sh start historyserver
</code></pre></div>
<p>6) We can test that everything when fine by taking the following actions:</p>

<ul>
<li> Running on <code>jps</code> on the master node and slave nodes, which should display the corresponding processes. You will need to instal java-devel first (`yum install java-devel).</li>
<li> We can put any file on the dfs from any node by running:</li>
</ul>
<div class="highlight"><pre><code class="language-text" data-lang="text">hdfs dfs -mkdir /test_dir
hdfs dfs -put $any_file_in_local_storage /test_dir/$file_name
</code></pre></div>
<p>And retrieving them in any other node by running <code>hdfs dfs -get /test_dir/$file_name $file_name</code>.</p>

<ul>
<li>Finally, we are ready to run a the popular WordCount MapReduce example. For example, save a text file &quot;hello_world.txt&quot; with the words &quot;hello world hello Hello&quot;. Then, run these commands:</li>
</ul>
<div class="highlight"><pre><code class="language-text" data-lang="text">hdfs dfs -mkdir /hello_world_in
</code></pre></div><div class="highlight"><pre><code class="language-text" data-lang="text">hdfs dfs -put hello_world.txt /helloWorld_input/hello_world.txt
</code></pre></div><div class="highlight"><pre><code class="language-text" data-lang="text">hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.4.jar wordcount /hello_world_in /hello_world_out
</code></pre></div>
<p><code>
hdfs dfs -cat /hello_world_out/part*
</code>`</p>

<p>You should see in stdout:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">Hello    1
hello    2
world    1
</code></pre></div>
<h3>1.2. Nutch</h3>

<p>7) Download Apache Nutch 1.12 source from [14] to your &quot;master&quot; t2.micro instance. Good guides to follow are [15-16], although please note that step 12 is now different than in those resources.</p>

<p>8) Configure <code>$NUTCH_HOME/conf/nutch-site.xml</code> according to [4]. At this point, the only mandatory property is <code>http.agent.name</code>, which can be set to any arbitrary value.</p>

<p>9) For my example test case, in which I will only crawl this very same webpage, <code>$NUTCH_HOME/conf/regex-urlfilter.txt</code> also has to be edited with a regular expression matching all pages under <code>http://mario-lopeztelecom.github.io/</code>. Right below &quot;Accept everything else&quot;, change the existing expression to:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">+^http://mario-lopeztelecom.github.io/.*``
</code></pre></div>
<p>10) Copy the following files from the Hadoop configuration to <code>$NUTCH_HOME/conf</code>: <code>hadoop-env.sh, core-site.xml, hdfs-site.xml, mapred-site.xml, slaves</code>.</p>

<p>11) Install ant in order to compile Nutch (<code>yum install ant</code>), and run <code>ant runtime</code> on <code>$NUTCH_HOME</code>.</p>

<p>12) To test the crawl (without indexing), create a file <code>seeds.txt</code>, put in the dfs (<em>e.g.,</em> in <code>/urls</code> folder), and then run:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$NUTCH_HOME/runtime/deploy/bin /urls /crawlOutput $rounds
</code></pre></div>
<p>where `$rounds is the number of rounds you want the crawler to run for.</p>

<h3>1.3. Elasticsearch</h3>

<p>13) Download Elasticsearch 1.4.1 from [17], both on the &quot;master&quot; and &quot;slave1&quot; t2.micro instances.</p>

<p>14) On <code>$ELASTIC_HOME/config/elasticsearch.yml</code> @ slave1, made the following changes to be able to form a cluster:</p>

<ul>
<li> Uncomment <code>#cluster.name: elasticsearch</code>: the slave1 instance of elasticsearch will join the existing cluster &quot;elasticsearch&quot; created by the master node.</li>
<li> <code>discovery.zen.ping.multicast.enabled: false</code>.</li>
<li> <code>discovery.zen.ping.unicast.hosts: [$address_of_master]</code>.</li>
</ul>

<p>15) At this point you should be able to start both instances of elasticsearch (running <code>$ELASTIC_HOME/bin/elasticsearch</code> first on master, then on slave1) and see in the stdout that they form a cluster. With the standard config we have not touched, each instance should hold a replica of the data we store in elasticsearch. </p>

<p>16) Create the (intially empty) index for the crawl results [18]. For example, we will create &quot;test_index&quot;:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">curl -XPUT &#39;localhost:9200/test_index?pretty&#39;
</code></pre></div>
<p>17) If you have not done it already on step 8 (if you did not copied the whole contents of <code>nutch-site.xml</code> from [4], leaving out the ones related to elasticsearch), now it is the time to do so. These properties are pretty self-explanatory, only note that <code>elastic.host</code> refers to the address of the master instance of the elasticsearch cluster. Link [19] provides more info on these parameters. Also remember to recompile Nutch after doing these changes.</p>

<p>18) Finally, we can launch the whole crawl and indexing process with: </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$NUTCH_HOME/bin/crawl -i  /urls /crawlOutput  2
</code></pre></div>
<p>where the &quot;-i&quot; switch is for indexing on elasticsearch. If you have run the crawl first, and you only want to index the results of a particular segment, use the following command:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$NUTCH_HOME/runtime/deploy/bin/nutch index /crawlOutput//crawldb -linkdb /crawlOutput//linkdb /crawlOutput//segments/$segment
</code></pre></div>
<p>19) Test the result. We should be able to see the crawled contents on the elasticsearch index by running: </p>
<div class="highlight"><pre><code class="language-text" data-lang="text">curl &#39;localhost:9200/test_index/_search?q=*&amp;pretty&#39;
</code></pre></div>
<p>and you will see something similar to this:
<img style="display:block;margin:auto" src="/pics/2016-06-26/screen.png" alt=""/></p>

<h1>2. Discussion on major difficulties and decisions made.</h1>

<h3>2.1 Architecture</h3>

<p><strong>2.1.1. Decision #1. How to split the required Hadoop roles.</strong></p>

<p>The decision of having a master coordinator-only node instead of three worker nodes where one also holds the master role, came from the intuition that t2.micro instances, with 1GB of RAM, were going to struggle even under moderate workload. The memory issues I had later, as well as this StackOverflow post [8], confirmed this intuition.</p>

<p><strong>2.1.2. Decision #2. Elasticsearch cluster.</strong></p>

<p>Facing the previously mentioned memory issue, I was not sure on which node will be harmed the most by also holding an Elasticsearch node. Because of that, for experimentation purposes, I decided to spin up an Elasticsearch node both on my &quot;master&quot; t2.micro instance and on one worker node, but not on the other (so that at least one node has a lower workload and a chance to survive a crash.) Ideally, I would have put elasticsearch on a separate t2.micro instance, but that would have got me out of the free tier.</p>

<h3>2.2. Configuration: memory issues</h3>

<p>This has been, by far, the most difficult problem to solve. When running the whole crawl and indexing process, I could see lots of failed map tasks in the ApplicationTracker. In fact, many times, the job itself failed, DataNodes crashed, and/or elasticsearch was killed. </p>

<p><strong>2.2.1. Identifying the issue</strong></p>

<p>Both examples of wordcount, as well as all the phases of Nutch crawl seem to run without errors (in this latter case, I could still see some failed map and reduce tasks). However, the indexing step of Nutch failed almost always. Fortunately, I could just run the indexing step on an existing segment from a previous crawl, with:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">$NUTCH_HOME/runtime/deploy/bin/nutch index /crawlOutput//crawldb -linkdb /crawlOutput//linkdb /crawlOutput//segments/20160623170331
</code></pre></div>
<p>where <code>crawlOutput</code> is the folder in the dfs that held the segments of the Nutch crawl.</p>

<p>The stack traces I could see in the terminal where I launched the application, as well as in the ResourceManager web UI (or in the JobHistory Server) were not informative most of the times:</p>
<div class="highlight"><pre><code class="language-text" data-lang="text">Exit code: 1
Stack trace: ExitCodeException exitCode=1:
at org.apache.hadoop.util.Shell.runCommand(Shell.java:538)
at org.apache.hadoop.util.Shell.run(Shell.java:455)
at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:715)
at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
at java.util.concurrent.FutureTask.run(FutureTask.java:262)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)
</code></pre></div>
<p>But, at some point I realized that, randomly, one of these things happened:</p>

<ul>
<li> The elasticsearch node process was killed and I could see <code>org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available</code>. If I tried to start it again, I could even see on the terminal: <code>java.lang.OutOfMemoryError</code>.</li>
<li> <code>Error: unable to create new native thread Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 Container exited with a non-zero exit code 143</code>.</li>
<li> &quot;Slave1&quot; t2.micro instance disappeared from the &quot;Nodes&quot; page on the ResourceManager web UI (and running <code>jps</code> on its terminal will show no sign of the NodeManager running).</li>
</ul>

<p>Joining all this together made me quickly realize it was a memory issue.</p>

<p><strong>2.2.2. The issue</strong></p>

<p>The affected nodes run out of memory, and the OS killed one of the most consuming processes, which sometimes was the elasticsearch process, and sometimes a Hadoop process [9]. </p>

<p><strong>2.2.3. The solution</strong></p>

<p>I reached the solution in two steps. First, this StackOverflow post allowed me to reach this page [10] about Hadoop memory configuration. Doing a quick check on the default values for properties such as <code>yarn.nodemanager.resource.memory-mb</code> [11] made me realize that they were unsuitable for a t2.micro instance (the default value of that property, which indicates the amount of physical memory that can be allocated for containers, was set to 8GB...). Thus, I changed my values according to [10].</p>

<p>This was not enough, however. Now the tasks were never allocated to any node and the jobs were never completed. It turns out there was also a need to restrict the amount of memory of the MR AppMaster containers [12], thus, accordingly, I set appropriate values for <code>yarn.app.mapreduce.am.resource.mb</code> and <code>yarn.app.mapreduce.am.command-opts</code> [13]. And this last fix allowed me to run all tasks without errors.</p>

<p><strong>2.2.4. Still to do</strong>
I believe that there would be more properties to tweak in order to achieve a higher performance. For example, I set <code>yarn.scheduler.minimum-allocation-mb</code> to the same value of <code>yarn.nodemanager.resource.memory-mb</code>, which means that only one container is allowed on a worker node, reserving all the available memory (even if the task does not require such amount of memory.). This allows me to be on the safe side regarding memory limits, but for low memory tasks, is results in under-utilization of resources.</p>

<h1>3. Minor discussions.</h1>

<p>The following comments are related to the respective steps of the mini-guide on section 1.</p>

<p>1- We can only spin up 3 t2.micro instances since the minimal amount of disk that can be set is 8GB, but more than 30GB falls out of the free tier [2]. I chose Amazon Linux AMI thinking that it may already contain certain prerequisites (Java SDK for example). In a production environment, different considerations may apply.</p>

<p>2- In a production environment, security rules should be as restrictive as possible.</p>

<p>7- I chose Nutch 1.x over 2.x since I do not need the added features of 2.x and it seems to be harder to configure. We cannot use the binaries here, because we want it to run on top of our existing Hadoop cluster, and thus, the config files of our Hadoop installation will take part on the compilation process of Nutch. </p>

<p>13- Unfortunately, Amazon Elasticsearch Service [15] cannot be used, and we must install and configure Elasticsearch by ourselves. The reason behind this is that such service only supports port 80 (the REST API to elasticsearch), whereas the elasticsearch plugin for Nutch uses the &quot;Transport Client&quot; which needs port 9300 (by default) [16].</p>

<p>14- For some unknown reason, the &quot;automatic cluster discovery&quot; of elasticsearch did not work for slave1 to automatically join master. Forcing unicast discovery did the trick.</p>

<h1>4. References</h1>

<div>
[1] <a href=http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html>http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html</a><br>
[2] <a href=https://aws.amazon.com/es/free>https://aws.amazon.com/es/free</a><br>
[3] <a href=http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz>http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.6.4/hadoop-2.6.4.tar.gz</a><br>
[4] <a href=https://gist.github.com/Mario-LopezTelecom/84a191bf76550ec1846d34b8e6d53595>https://gist.github.com/Mario-LopezTelecom/84a191bf76550ec1846d34b8e6d53595</a><br>
[5] <a href=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html>http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html</a><br>
[6] <a href=http://www.allprogrammingtutorials.com/tutorials/setting-up-hadoop-2-6-0-cluster.php>http://www.allprogrammingtutorials.com/tutorials/setting-up-hadoop-2-6-0-cluster.php</a><br>  
[7] <a href=http://arturmkrtchyan.com/how-to-setup-multi-node-hadoop-2-yarn-cluster>http://arturmkrtchyan.com/how-to-setup-multi-node-hadoop-2-yarn-cluster</a><br>  
[8] <a href=http://stackoverflow.com/questions/29600370/hadoop-2-x-on-amazon-ec2-t2-micro>http://stackoverflow.com/questions/29600370/hadoop-2-x-on-amazon-ec2-t2-micro</a><br>  
[9] <a href=http://stackoverflow.com/questions/29001702/why-yarn-java-heap-space-memory-error>http://stackoverflow.com/questions/29001702/why-yarn-java-heap-space-memory-error</a><br>  
[10] <a href=http://hortonworks.com/blog/how-to-plan-and-configure-yarn-in-hdp-2-0/>http://hortonworks.com/blog/how-to-plan-and-configure-yarn-in-hdp-2-0</a><br>  
[11] <a href=http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml>http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</a><br>  
[12] <a href=http://stackoverflow.com/questions/34467308/mapreduce-job-hangs-waiting-for-am-container-to-be-allocated>http://stackoverflow.com/questions/34467308/mapreduce-job-hangs-waiting-for-am-container-to-be-allocated</a><br>  
[13] <a href=https://gist.github.com/Mario-LopezTelecom/84a191bf76550ec1846d34b8e6d53595#file-mapred-site-xml-L41>https://gist.github.com/Mario-LopezTelecom/84a191bf76550ec1846d34b8e6d53595#file-mapred-site-xml-L41</a><br> 
[14] <a href=http://www.apache.org/dyn/closer.lua/nutch/1.12/apache-nutch-1.12-src.tar.gz>http://www.apache.org/dyn/closer.lua/nutch/1.12/apache-nutch-1.12-src.tar.gz</a><br>
[15] <a href=https://aws.amazon.com/es/elasticsearch-service/>https://aws.amazon.com/es/elasticsearch-service</a><br>
[16] <a href=https://forums.aws.amazon.com/thread.jspa?messageID=681938>https://forums.aws.amazon.com/thread.jspa?messageID=681938</a><br>
[17] <a href=https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.4.1.tar.gz>https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-1.4.1.tar.gz</a><br>
[18] <a href=https://www.elastic.co/guide/en/elasticsearch/reference/1.4/_create_an_index.html>https://www.elastic.co/guide/en/elasticsearch/reference/1.4/_create_an_index.html</a><br>
[19] <a href=https://www.mind-it.info/2013/09/26/integrating-nutch-1-7-elasticsearch/>https://www.mind-it.info/2013/09/26/integrating-nutch-1-7-elasticsearch</a><br>
</div>

</div>

<div class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2016/03/14/acho-que-has-estado-haciendo/">
            Acho, ¿qué has estado haciendo estos últimos tres años?
            <small>14 Mar 2016</small>
          </a>
        </h3>
      </li>
    
  </ul>
</div>

    </div>

  </body>
</html>
